{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjueoEFbZmmp"
      },
      "source": [
        "# USAGE\n",
        "# python detect_age.py --image images/adrian.png --face face_detector --age age_detector\n",
        "\n",
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img=input(\"Enter the image Name\")\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "\n",
        "\n",
        "# define the list of age buckets our age detector will predict\n",
        "AGE_BUCKETS = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\",\n",
        "\t\"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath =\"deploy.prototxt\"\n",
        "weightsPath = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load our serialized age detector model from disk\n",
        "print(\"[INFO] loading age detector model...\")\n",
        "prototxtPath = \"age_deploy.prototxt\"\n",
        "weightsPath = \"age_net.caffemodel\"\n",
        "ageNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the input image and construct an input blob for the image\n",
        "image = cv2.imread(img)\n",
        "(h, w) = image.shape[:2]\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0))\n",
        "\n",
        "# pass the blob through the network and obtain the face detections\n",
        "print(\"[INFO] computing face detections...\")\n",
        "faceNet.setInput(blob)\n",
        "detections = faceNet.forward()\n",
        "\n",
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# extract the confidence (i.e., probability) associated with the\n",
        "\t# prediction\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t# filter out weak detections by ensuring the confidence is\n",
        "\t# greater than the minimum confidence\n",
        "\tif confidence > 0.5:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for the\n",
        "\t\t# object\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t# extract the ROI of the face and then construct a blob from\n",
        "\t\t# *only* the face ROI\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0, (227, 227),\n",
        "\t\t\t(78.4263377603, 87.7689143744, 114.895847746),\n",
        "\t\t\tswapRB=False)\n",
        "\n",
        "\t\t# make predictions on the age and find the age bucket with\n",
        "\t\t# the largest corresponding probability\n",
        "\t\tageNet.setInput(faceBlob)\n",
        "\t\tpreds = ageNet.forward()\n",
        "\t\ti = preds[0].argmax()\n",
        "\t\tage = AGE_BUCKETS[i]\n",
        "\t\tageConfidence = preds[0][i]\n",
        "\n",
        "\t\t# display the predicted age to our terminal\n",
        "\t\ttext = \"{}: {:.2f}%\".format(age, ageConfidence * 100)\n",
        "\t\tprint(\"[INFO] {}\".format(text))\n",
        "\n",
        "\t\t# draw the bounding box of the face along with the associated\n",
        "\t\t# predicted age\n",
        "\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
        "\t\t\t(0, 0, 255), 2)\n",
        "\t\tcv2.putText(image, text, (startX, y),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "\n",
        "# display the output image\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask-bootstrap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVURfjIklVAJ",
        "outputId": "8860c388-c847-4fa6-8d4e-ead71ccf2eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting flask-bootstrap\n",
            "  Downloading Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-bootstrap) (1.1.4)\n",
            "Collecting dominate\n",
            "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visitor\n",
            "  Downloading visitor-0.1.3.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-bootstrap) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-bootstrap) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-bootstrap) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-bootstrap) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-bootstrap) (2.0.1)\n",
            "Building wheels for collected packages: flask-bootstrap, visitor\n",
            "  Building wheel for flask-bootstrap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flask-bootstrap: filename=Flask_Bootstrap-3.3.7.1-py3-none-any.whl size=460123 sha256=d071336e8a7b9bc993cf1906aaae75037922fb29566d3238a0bf51bdbcf636f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/a2/d6/50d039c9b59b4caca6d7b53839c8100354a52ab7553d2456eb\n",
            "  Building wheel for visitor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visitor: filename=visitor-0.1.3-py3-none-any.whl size=3946 sha256=475e491bbfb25663407ba1d0e8cb0cb458b7a2f5312673d49935e26d480d976c\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/34/11/053f47218984c9a31a00f911ed98dda036b867481dcc527a12\n",
            "Successfully built flask-bootstrap visitor\n",
            "Installing collected packages: visitor, dominate, flask-bootstrap\n",
            "Successfully installed dominate-2.6.0 flask-bootstrap-3.3.7.1 visitor-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz1hSJY1awJa",
        "outputId": "407ea8fa-f9e5-43ed-a30e-5d5c22e2d722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "# USAGE\n",
        "# python detect_age_video.py --face face_detector --age age_detector\n",
        "\n",
        "# import the necessary packages\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def detect_and_predict_age(frame, faceNet, ageNet, minConf=0.5):\n",
        "\t# define the list of age buckets our age detector will predict\n",
        "\tAGE_BUCKETS = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\",\n",
        "\t\t\"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
        "\n",
        "\t# initialize our results list\n",
        "\tresults = []\n",
        "\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the prediction\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence > minConf:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# extract the ROI of the face\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\n",
        "\t\t\t# ensure the face ROI is sufficiently large\n",
        "\t\t\tif face.shape[0] < 20 or face.shape[1] < 20:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# construct a blob from *just* the face ROI\n",
        "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0, (227, 227),\n",
        "\t\t\t\t(78.4263377603, 87.7689143744, 114.895847746),\n",
        "\t\t\t\tswapRB=False)\n",
        "\n",
        "\t\t\t# make predictions on the age and find the age bucket with\n",
        "\t\t\t# the largest corresponding probability\n",
        "\t\t\tageNet.setInput(faceBlob)\n",
        "\t\t\tpreds = ageNet.forward()\n",
        "\t\t\ti = preds[0].argmax()\n",
        "\t\t\tage = AGE_BUCKETS[i]\n",
        "\t\t\tageConfidence = preds[0][i]\n",
        "\n",
        "\t\t\t# construct a dictionary consisting of both the face\n",
        "\t\t\t# bounding box location along with the age prediction,\n",
        "\t\t\t# then update our results list\n",
        "\t\t\td = {\n",
        "\t\t\t\t\"loc\": (startX, startY, endX, endY),\n",
        "\t\t\t\t\"age\": (age, ageConfidence)\n",
        "\t\t\t}\n",
        "\t\t\tresults.append(d)\n",
        "\n",
        "\t# return our results to the calling function\n",
        "\treturn results\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = \"deploy.prototxt\"\n",
        "weightsPath = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load our serialized age detector model from disk\n",
        "print(\"[INFO] loading age detector model...\")\n",
        "prototxtPath = \"age_deploy.prototxt\"\n",
        "weightsPath = \"age_net.caffemodel\"\n",
        "ageNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# initialize the video stream and allow the camera sensor to warm up\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=1).start()\n",
        "time.sleep(2.0)\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 400 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=300)\n",
        "\n",
        "\t# detect faces in the frame, and for each face in the frame,\n",
        "\t# predict the age\n",
        "\tresults = detect_and_predict_age(frame, faceNet, ageNet,\n",
        "\t\tminConf=args[\"confidence\"])\n",
        "\n",
        "\t# loop over the results\n",
        "\tfor r in results:\n",
        "\t\t# draw the bounding box of the face along with the associated\n",
        "\t\t# predicted age\n",
        "\t\ttext = \"{}: {:.2f}%\".format(r[\"age\"][0], r[\"age\"][1] * 100)\n",
        "\t\t(startX, startY, endX, endY) = r[\"loc\"]\n",
        "\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "\t\t\t(0, 0, 255), 2)\n",
        "\t\tcv2.putText(frame, text, (startX, y),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "\n",
        "\t# show the output frame\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\t\t\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading face detector model...\n",
            "[INFO] loading age detector model...\n",
            "[INFO] starting video stream...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a8430512514a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# to have a maximum width of 400 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# detect faces in the frame, and for each face in the frame,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB67DIJ0bCQV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSINJSwc3Qbq"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfUXM8lC3ShS"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-oXeU4H3SgN"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsDnfzz93QZV"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}